{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad5563ba-b8d9-46fd-a485-ceb4735e2c21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dans ce TP, nous considérons des trajets en vélo partagé (similaire au vélib) en Californie. Deux jeux de données sont fournis : l'un qui contient les stations de vélo, l'autre, les trajets à vélo. Les déplacements à vélo se font d'une station à l'autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a14dbd23-51f9-4f52-bbe1-f4f01f11e030",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf7dc796-dd34-42c2-8c22-93f6b85ad47c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Charger le fichier stationData.csv dans un DataFrame station_df et le fichier tripData.csv dans un DataFrame trip_df. Pour chaque Dataframe, il vous faudra demander une inférence des schémas et indiquer que la première ligne est un en-tête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436bbfc0-b90d-4202-8e9f-55099321d9ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "station_df = spark.read.option(\n",
    "    \"header\", \"true\"\n",
    ").option(\n",
    "    \"inferSchema\", \"true\"\n",
    ").csv(\"/FileStore/tables/stationData.csv\")\n",
    "\n",
    "station_df_2 = spark.read.options(\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").csv(\"/FileStore/tables/stationData.csv\")\n",
    "\n",
    "trip_df = spark.read.option(\n",
    "    \"header\", \"true\"\n",
    ").option(\n",
    "    \"inferSchema\", \"true\"\n",
    ").csv(\"/FileStore/tables/tripData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40f46af-d0c5-4926-85d4-f8f92b1d373e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=8519688365559781#setting/sparkui/0606-120751-o2kkv5i7/driver-7343896107068438337\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=8519688365559781#setting/sparkui/0606-120751-o2kkv5i7/driver-7343896107068438337\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7af2f8ad-32d3-4d66-9862-b0d2c55e15e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Afficher les schémas des 2 DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2e4af7-8c45-4a58-9965-f600074ca648",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- station_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- lat: double (nullable = true)\n |-- long: double (nullable = true)\n |-- dockcount: integer (nullable = true)\n |-- landmark: string (nullable = true)\n |-- installation: date (nullable = true)\n\nroot\n |-- TripID: integer (nullable = true)\n |-- Duration: integer (nullable = true)\n |-- StartDate: string (nullable = true)\n |-- StartStation: string (nullable = true)\n |-- StartTerminal: integer (nullable = true)\n |-- EndDate: string (nullable = true)\n |-- EndStation: string (nullable = true)\n |-- EndTerminal: integer (nullable = true)\n |-- Bike#: integer (nullable = true)\n |-- SubscriberType: string (nullable = true)\n |-- ZipCode: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "station_df_2.printSchema()\n",
    "trip_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b35305b3-5b4b-411a-80dd-9315b3b8c743",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Créer une vue pour chaque DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efee527c-afbe-413d-88f6-d75be200eee3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "station_df.createOrReplaceTempView(\"station_view\")\n",
    "trip_df.createOrReplaceTempView(\"trip_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cb92e5c-797b-4cad-8392-722d513cfb38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Trouver deux façons de calculer le nombre de trajets, l'une en appelant une méthode sur trip_df directement, l'autre en rédigeant une requête SQL de la vue correspondant au DataFrame tripData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50b63fd-c346-4158-bc06-3b8100194f24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354152\n+--------+\n|count(1)|\n+--------+\n|  354152|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "print(trip_df.count())\n",
    "spark.sql(\"\"\"SELECT count(*) FROM trip_view\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88e8bd91-78fc-4248-8066-ac6b3ece7dfb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ecrire une requête permettant de compter le nombre de trajets qui démarrent et se terminent à la même station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6365c23a-ec09-4ff6-a926-3d8e630c1598",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|   10276|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT count(*) FROM trip_view WHERE StartStation=EndStation\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb480a36-196d-4d88-bd35-12b4ab058136",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "On souhaite désormais obtenir l’id des stations associées à ces trajets. Ecrire une requête renvoyant la liste des terminaux concernés ainsi que le nombre de trajets pour chacun de ces terminaux. Trier le résultat par ordre décroissant de nombre de trajets.\n",
    "<br>Exemple de sortie :\n",
    "<br>+--------+--------+\n",
    "<br>|terminal|count(1)|\n",
    "<br>+--------+--------+\n",
    "<br>| 60| 850|\n",
    "<br>| 50| 708|\n",
    "<br>| 35| 348|\n",
    "<br>| 76| 320|\n",
    "<br>| 74| 307|\n",
    "<br>(La station 60 est la plus concernée par ces trajets cycliques, avec 850 de ces trajets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c766cd-b7b6-48bf-a516-89eb0ca26aed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n|terminal|count(1)|\n+--------+--------+\n|      60|     850|\n|      50|     708|\n|      35|     348|\n|      76|     320|\n|      74|     307|\n|      39|     296|\n|      61|     280|\n|      67|     277|\n|      71|     268|\n|      70|     260|\n|      28|     254|\n|      48|     248|\n|      54|     230|\n|      69|     227|\n|      42|     213|\n|      73|     200|\n|      57|     197|\n|      64|     194|\n|       3|     189|\n|      72|     181|\n+--------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT StartTerminal as terminal, count(*)\n",
    "    FROM trip_view\n",
    "    WHERE StartStation=EndStation\n",
    "    GROUP BY terminal\n",
    "    ORDER BY count(*) DESC\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53f2a5e-c08c-4702-8633-a98eb12df159",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n|StartTerminal|count|\n+-------------+-----+\n|           60|  850|\n|           50|  708|\n|           35|  348|\n|           76|  320|\n|           74|  307|\n|           39|  296|\n|           61|  280|\n|           67|  277|\n|           71|  268|\n|           70|  260|\n|           28|  254|\n|           48|  248|\n|           54|  230|\n|           69|  227|\n|           42|  213|\n|           73|  200|\n|           57|  197|\n|           64|  194|\n|            3|  189|\n|           72|  181|\n+-------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "cyclic_count = trip_df.select(\"StartTerminal\").filter(col(\"StartStation\") == col(\"EndStation\")).groupBy(\"StartTerminal\").count().orderBy(desc(\"count\"))\n",
    "cyclic_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5512115f-f2b2-4fef-a466-0847a7ea61c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dans la requête précédente, nous avons oublié un élément qui nous importe. Nous souhaitons compléter le résultat en indiquant le nombre de docks (dockcount) des stations concernées.\n",
    "<br>Exemple de sortie :\n",
    "<br>+--------+---------+--------+\n",
    "<br>|terminal|dockcount|count(1)|\n",
    "<br>+--------+---------+--------+\n",
    "<br>| 60| 15| 850|\n",
    "<br>| 50| 23| 708|\n",
    "<br>| 35| 11| 348|\n",
    "<br>| 76| 19| 320|\n",
    "<br>| 74| 23| 307|\n",
    "<br>Mettre à jour la requête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4a2b7f-a932-4575-bdc4-d15593a184f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85ca6c3d-0c56-4e48-9a52-d5e726699e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Rédiger les 2 requêtes précédentes avec le DSL de DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c956dd4-dab0-4c4e-94f8-5c46cf63f004",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b61df3b-6e86-4145-a96d-92cba40f5f3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+\n|StartTerminal|dockCount|count|\n+-------------+---------+-----+\n|           60|       15|  850|\n|           50|       23|  708|\n|           35|       11|  348|\n|           76|       19|  320|\n|           74|       23|  307|\n|           39|       19|  296|\n|           61|       27|  280|\n|           67|       27|  277|\n|           71|       19|  268|\n|           70|       19|  260|\n|           28|       23|  254|\n|           48|       15|  248|\n|           54|       15|  230|\n|           69|       23|  227|\n|           42|       15|  213|\n|           73|       15|  200|\n|           57|       15|  197|\n|           64|       15|  194|\n|            3|       15|  189|\n|           72|       23|  181|\n+-------------+---------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "res = trip_df.join(\n",
    "  station_df,\n",
    "  col(\"StartTerminal\") == col(\"station_id\"),\n",
    ").select(\n",
    "  \"StartTerminal\",\n",
    "  \"dockCount\",\n",
    ").filter(\n",
    "  col(\"StartStation\") == col(\"EndStation\")\n",
    ").groupBy(\n",
    "  \"StartTerminal\",\n",
    "  \"dockCount\"\n",
    ").count(\n",
    ").orderBy(\n",
    "  desc(\"count\")\n",
    ")\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c25a1c70-14b9-48dd-9f8d-96e8d6e7eec2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Observer le plan d’exécution des requêtes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b85a3c-55f6-42dc-be19-a0099d957fa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   Sort [count#629L DESC NULLS LAST], true, 0\n   +- Exchange rangepartitioning(count#629L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=1600]\n      +- Project [StartTerminal#346, dockCount#284, count(1)#628L AS count#629L]\n         +- HashAggregate(keys=[StartTerminal#346, dockCount#284], functions=[finalmerge_count(merge count#643L) AS count(1)#628L])\n            +- ShuffleQueryStage 1, Statistics(sizeInBytes=1971.5 MiB)\n               +- Exchange hashpartitioning(StartTerminal#346, dockCount#284, 200), ENSURE_REQUIREMENTS, [plan_id=1594]\n                  +- *(2) HashAggregate(keys=[StartTerminal#346, dockCount#284], functions=[merge_count(merge count#643L) AS count#643L])\n                     +- *(2) Project [StartTerminal#346, dockCount#284, count#643L]\n                        +- *(2) BroadcastHashJoin [StartTerminal#346], [station_id#280], Inner, BuildRight, false, true\n                           :- *(2) HashAggregate(keys=[StartTerminal#346], functions=[partial_count(1) AS count#643L])\n                           :  +- *(2) Project [StartTerminal#346]\n                           :     +- *(2) Filter (((isnotnull(StartStation#345) AND isnotnull(EndStation#348)) AND (StartStation#345 = EndStation#348)) AND isnotnull(StartTerminal#346))\n                           :        +- FileScan csv [StartStation#345,StartTerminal#346,EndStation#348] Batched: false, DataFilters: [isnotnull(StartStation#345), isnotnull(EndStation#348), (StartStation#345 = EndStation#348), isn..., Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/tables/tripData.csv], PartitionFilters: [], PushedFilters: [IsNotNull(StartStation), IsNotNull(EndStation), IsNotNull(StartTerminal)], ReadSchema: struct<StartStation:string,StartTerminal:int,EndStation:string>\n                           +- ShuffleQueryStage 0, Statistics(sizeInBytes=1680.0 B, rowCount=70, isRuntime=true)\n                              +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=1415]\n                                 +- *(1) Filter isnotnull(station_id#280)\n                                    +- FileScan csv [station_id#280,dockcount#284] Batched: false, DataFilters: [isnotnull(station_id#280)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/tables/stationData.csv], PartitionFilters: [], PushedFilters: [IsNotNull(station_id)], ReadSchema: struct<station_id:int,dockcount:int>\n+- == Initial Plan ==\n   Sort [count#629L DESC NULLS LAST], true, 0\n   +- Exchange rangepartitioning(count#629L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=1250]\n      +- Project [StartTerminal#346, dockCount#284, count(1)#628L AS count#629L]\n         +- HashAggregate(keys=[StartTerminal#346, dockCount#284], functions=[finalmerge_count(merge count#643L) AS count(1)#628L])\n            +- Exchange hashpartitioning(StartTerminal#346, dockCount#284, 200), ENSURE_REQUIREMENTS, [plan_id=1246]\n               +- HashAggregate(keys=[StartTerminal#346, dockCount#284], functions=[merge_count(merge count#643L) AS count#643L])\n                  +- Project [StartTerminal#346, dockCount#284, count#643L]\n                     +- BroadcastHashJoin [StartTerminal#346], [station_id#280], Inner, BuildRight, false, true\n                        :- HashAggregate(keys=[StartTerminal#346], functions=[partial_count(1) AS count#643L])\n                        :  +- Project [StartTerminal#346]\n                        :     +- Filter (((isnotnull(StartStation#345) AND isnotnull(EndStation#348)) AND (StartStation#345 = EndStation#348)) AND isnotnull(StartTerminal#346))\n                        :        +- FileScan csv [StartStation#345,StartTerminal#346,EndStation#348] Batched: false, DataFilters: [isnotnull(StartStation#345), isnotnull(EndStation#348), (StartStation#345 = EndStation#348), isn..., Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/tables/tripData.csv], PartitionFilters: [], PushedFilters: [IsNotNull(StartStation), IsNotNull(EndStation), IsNotNull(StartTerminal)], ReadSchema: struct<StartStation:string,StartTerminal:int,EndStation:string>\n                        +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=1241]\n                           +- Filter isnotnull(station_id#280)\n                              +- FileScan csv [station_id#280,dockcount#284] Batched: false, DataFilters: [isnotnull(station_id#280)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/tables/stationData.csv], PartitionFilters: [], PushedFilters: [IsNotNull(station_id)], ReadSchema: struct<station_id:int,dockcount:int>\n\n\n"
     ]
    }
   ],
   "source": [
    "res.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0362ad36-3c8a-4573-87fc-8a95ba66c398",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-prise-en-main_live-coding_spark-sql-dsl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
